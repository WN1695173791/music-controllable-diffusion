
Training GMVAE
Training --> GMIWVae(
  (_encoder): Encoder(
    (_net): Sequential(
      (0): Linear(in_features=784, out_features=392, bias=True)
      (1): ReLU()
      (2): Linear(in_features=392, out_features=196, bias=True)
      (3): Tanh()
      (4): Dropout(p=0.5, inplace=False)
      (5): Linear(in_features=196, out_features=196, bias=True)
      (6): ReLU()
      (7): Linear(in_features=196, out_features=98, bias=True)
    )
    (_fc_mean): Sequential(
      (0): Linear(in_features=98, out_features=20, bias=True)
    )
    (_fc_log_var): Sequential(
      (0): Linear(in_features=98, out_features=20, bias=True)
    )
  )
  (_decoder): Decoder(
    (_net): Sequential(
      (0): Linear(in_features=20, out_features=196, bias=True)
      (1): ReLU()
      (2): Linear(in_features=196, out_features=392, bias=True)
      (3): Tanh()
      (4): Linear(in_features=392, out_features=196, bias=True)
      (5): ReLU()
      (6): Linear(in_features=196, out_features=784, bias=True)
      (7): Sigmoid()
    )
  )
)
====> Train Loss = 0.0019759204282114905 Epoch = 1
====> Train Loss = 0.0013585979017118612 Epoch = 2
====> Train Loss = 0.001329615162188808 Epoch = 3
====> Train Loss = 0.0012480252207567293 Epoch = 4
====> Train Loss = 0.0012284738402813674 Epoch = 5
====> Train Loss = 0.0012211658625553051 Epoch = 6
====> Train Loss = 0.0011953195855021477 Epoch = 7
====> Train Loss = 0.0011784675773233175 Epoch = 8
====> Train Loss = 0.0011822317040214936 Epoch = 9
====> Train Loss = 0.0011686198881516853 Epoch = 10
====> Train Loss = 0.0011592339877039195 Epoch = 11
====> Train Loss = 0.0011801657718916734 Epoch = 12
====> Train Loss = 0.001139081514502565 Epoch = 13
====> Train Loss = 0.0011644506841897964 Epoch = 14
====> Train Loss = 0.001145055548598369 Epoch = 15
====> Train Loss = 0.0011456330664455891 Epoch = 16
====> Train Loss = 0.0011506740398705005 Epoch = 17
====> Train Loss = 0.001137819943204522 Epoch = 18

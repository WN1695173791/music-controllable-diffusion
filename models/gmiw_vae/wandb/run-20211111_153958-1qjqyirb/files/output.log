
Training GMVAE
Training --> GMIWVae(
  (_encoder): Encoder(
    (_net): Sequential(
      (0): Linear(in_features=784, out_features=392, bias=True)
      (1): ReLU()
      (2): Linear(in_features=392, out_features=196, bias=True)
      (3): Tanh()
      (4): Dropout(p=0.5, inplace=False)
      (5): Linear(in_features=196, out_features=196, bias=True)
      (6): ReLU()
      (7): Linear(in_features=196, out_features=98, bias=True)
    )
    (_fc_mean): Sequential(
      (0): Linear(in_features=98, out_features=20, bias=True)
    )
    (_fc_log_var): Sequential(
      (0): Linear(in_features=98, out_features=20, bias=True)
    )
  )
  (_decoder): Decoder(
    (_net): Sequential(
      (0): Linear(in_features=20, out_features=196, bias=True)
      (1): ReLU()
      (2): Linear(in_features=196, out_features=392, bias=True)
      (3): Tanh()
      (4): Linear(in_features=392, out_features=196, bias=True)
      (5): ReLU()
      (6): Linear(in_features=196, out_features=784, bias=True)
      (7): Sigmoid()
    )
  )
)
====> Train Loss = -7.588607347415386e+21 Epoch = 1
====> Train Loss = -6.681139251569492e+21 Epoch = 2
====> Train Loss = -5.54122898871069e+21 Epoch = 3
====> Train Loss = -6.155800989724888e+21 Epoch = 4
====> Train Loss = -6.674361547238522e+21 Epoch = 5
====> Train Loss = -8.028051827526839e+21 Epoch = 6
====> Train Loss = -5.124414200240912e+21 Epoch = 7
====> Train Loss = -5.628383032156882e+21 Epoch = 8
====> Train Loss = -8.771044010387711e+21 Epoch = 9
====> Train Loss = -6.766956390793267e+21 Epoch = 10
====> Train Loss = -4.4818509426937834e+21 Epoch = 11
====> Train Loss = -5.375040440609537e+21 Epoch = 12
Python 3.8.10 (default, Sep 28 2021, 16:10:42)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.27.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.27.0
Out[1]: tensor(6.8820e+23, device='cuda:0', grad_fn=<SumBackward0>)
Out[2]:
tensor([-6.2074e+20, -1.2248e+21, -1.7450e+20, -3.7897e+20, -8.6371e+18,
        -4.3340e+16, -1.5621e+21, -1.6501e+20, -1.2540e+20, -7.4297e+20,
        -1.7299e+18, -2.1092e+21, -1.3562e+22, -1.2895e+19, -1.5487e+22,
        -6.1343e+18, -3.3972e+18, -2.9713e+19, -1.1201e+21, -3.6299e+19,
        -1.6223e+19, -1.7758e+17, -5.3850e+20, -2.6954e+21, -2.5552e+20,
        -4.2170e+21, -2.2710e+19, -7.5000e+20, -7.7653e+20, -2.6287e+20,
        -1.2307e+23, -1.9443e+18, -4.8313e+19, -4.6253e+18, -9.8026e+19,
        -5.8167e+19, -3.1821e+18, -6.8185e+16, -2.1155e+21, -2.8692e+21,
        -1.2472e+21, -1.3701e+21, -4.1730e+18, -2.2228e+19, -4.4352e+21,
        -1.0630e+20, -9.7341e+22, -4.6008e+18, -2.0931e+20, -3.1474e+19,
        -3.5852e+19, -1.2470e+20, -1.7958e+19, -1.4196e+18, -2.2756e+21,
        -9.0891e+19, -4.9275e+20, -3.2012e+21, -3.4074e+19, -1.8746e+21,
        -1.8884e+19, -3.1673e+20, -2.9318e+20, -1.0221e+19, -1.1642e+19,
        -1.7873e+21, -1.3453e+20, -5.8919e+20, -3.2576e+18, -6.4231e+17,
        -1.0306e+20, -4.1926e+19, -5.0438e+20, -4.7729e+22, -1.1185e+19,
        -7.7277e+21, -1.1307e+21, -3.1139e+19, -2.1754e+22, -4.2294e+19,
        -4.6380e+20, -1.3383e+20, -1.5637e+21, -1.6443e+21, -3.2324e+19,
        -4.6855e+16, -6.5287e+20, -6.7908e+21, -1.6897e+20, -1.3557e+21,
        -4.1099e+19, -8.1368e+20, -1.4584e+21, -8.7595e+19, -2.6827e+22,
        -3.6350e+19, -2.2716e+20, -1.5931e+20, -1.2099e+21, -1.5964e+20,
        -1.4010e+18, -1.3078e+16, -1.8549e+21, -2.6911e+21, -1.9935e+20,
        -1.1655e+23, -8.1902e+19, -5.4592e+21, -4.3662e+20, -1.8183e+20,
        -1.5500e+22, -2.9974e+20, -2.9029e+20, -3.1483e+18, -6.1195e+20,
        -1.7272e+19, -1.1231e+18, -1.9792e+17, -3.1705e+21, -1.8532e+21,
        -6.8005e+18, -4.0736e+22, -4.4868e+19, -1.0331e+21, -3.4690e+21,
        -3.6337e+19, -2.3048e+21, -1.3029e+20, -1.7796e+20, -5.2695e+20,
        -1.4625e+21, -2.4101e+20, -1.5263e+19, -5.9416e+17, -1.4950e+21,
        -4.3177e+20, -9.3216e+18, -5.3991e+21, -3.8832e+17, -4.4190e+21,
        -2.6185e+21, -7.8691e+20, -3.1204e+20, -6.1477e+20, -4.4806e+20,
        -5.8561e+20, -8.6230e+20, -9.0889e+18, -2.5013e+18, -1.2707e+18,
        -1.1457e+21, -1.7346e+21, -2.4766e+20, -5.3794e+22, -5.9004e+19,
        -2.4394e+20, -2.2044e+21, -3.5865e+19, -6.9355e+21, -4.9840e+20],

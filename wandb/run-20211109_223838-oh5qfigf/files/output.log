
Training simple VAE
/home/joy/.venv/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
Training --> SimpleVae(
  (_encoder): Encoder(
    (_net): Sequential(
      (0): Linear(in_features=784, out_features=392, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=392, out_features=392, bias=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Linear(in_features=392, out_features=196, bias=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=196, out_features=98, bias=True)
    )
    (_fc_mean): Sequential(
      (0): Linear(in_features=98, out_features=4, bias=True)
    )
    (_fc_log_var): Sequential(
      (0): Linear(in_features=98, out_features=4, bias=True)
    )
  )
  (_decoder): Decoder(
    (_net): Sequential(
      (0): Linear(in_features=4, out_features=196, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Linear(in_features=196, out_features=392, bias=True)
      (3): LeakyReLU(negative_slope=0.01)
      (4): Linear(in_features=392, out_features=392, bias=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=392, out_features=784, bias=True)
      (7): Sigmoid()
    )
  )
)
====> Train Loss = 46.23078079223633 Epoch = 1
====> Train Loss = 11.905058193206788 Epoch = 2
====> Train Loss = 5.785055446624756 Epoch = 3
====> Train Loss = 4.444841718673706 Epoch = 4
====> Train Loss = 2.809770441055298 Epoch = 5
====>  Test Loss = 2.68298602104187
====> Train Loss = 2.407502555847168 Epoch = 6
====> Train Loss = 1.72026846408844 Epoch = 7
====> Train Loss = 1.515982174873352 Epoch = 8
====> Train Loss = 1.3322008848190308 Epoch = 9
====> Train Loss = 1.0568965911865233 Epoch = 10
====>  Test Loss = 1.0062860250473022
====> Train Loss = 0.9650975227355957 Epoch = 11
====> Train Loss = 0.8481662273406982 Epoch = 12
====> Train Loss = 0.7499168872833252 Epoch = 13
====> Train Loss = 0.695289695262909 Epoch = 14
====> Train Loss = 0.6374652504920959 Epoch = 15
====>  Test Loss = 0.6141705513000488
====> Train Loss = 0.5979735612869262 Epoch = 16
====> Train Loss = 0.5643492221832276 Epoch = 17
====> Train Loss = 0.537263959646225 Epoch = 18
====> Train Loss = 0.5143067419528962 Epoch = 19
====> Train Loss = 0.494529926776886 Epoch = 20
====>  Test Loss = 0.48624199628829956
====> Train Loss = 0.47824899554252626 Epoch = 21
====> Train Loss = 0.46397989988327026 Epoch = 22
====> Train Loss = 0.4517208456993103 Epoch = 23
====> Train Loss = 0.44092541337013247 Epoch = 24
====> Train Loss = 0.4316291809082031 Epoch = 25
